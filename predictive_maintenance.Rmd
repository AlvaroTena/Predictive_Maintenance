---
title: "predictive_maintenance.Rmd"
output: html_document
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
if (!require(readr)) {
  install.packages("readr")
}
if (!require(dplyr)) {
  install.packages("dplyr")
}
if (!require(ggplot2)) {
  install.packages("ggplot2")
}
if (!require(GGally)) {
  install.packages("GGally")
}
if (!require(tidyr)) {
  install.packages("tidyr")
}
if (!require(plotly)) {
  install.packages("plotly")
}
if (!require(tidyverse)) {
  install.packages("tidyverse")
}
if (!require(ppcor)) {
  install.packages("ppcor")
}
if (!require(MASS)) {
  install.packages("MASS")
}
if (!require(pwr)) {
  install.packages("pwr")
}
if (!require(psych)) {
  install.packages("psych")
}
if (!require(corrplot)) {
  install.packages("corrplot")
}
if (!require(PerformanceAnalytics)) {
  install.packages("PerformanceAnalytics")
}
if (!require(lubridate)) {
  install.packages("lubridate")
}
if (!require(feasts)) {
  install.packages("feasts")
}
if (!require(reshape2)) {
  install.packages("reshape2")
}
if (!require(fable)) {
  install.packages("fable")
}
if (!require(forecast)) {
  install.packages("forecast")
}
if (!require(foreign)) {
  install.packages("foreign")
}
if (!require(fpp2)) {
  install.packages("fpp2")
}
if (!require(stats)) {
  install.packages("stats")
}
if (!require(tseries)) {
  install.packages("tseries")
}
if (!require(astsa)) {
  install.packages("astsa")
}
if (!require(quantmod)) {
  install.packages("quantmod")
}
if (!require(urca)) {
  install.packages("urca")
}

library(readr)
library(dplyr)
library(ggplot2)
library(GGally)
library(tidyr)
library(plotly)
library(tidyverse)
library(ppcor)
library(MASS)
library(pwr)
library(psych)
library(corrplot)
library(PerformanceAnalytics)
library(lubridate)
library(feasts)
library(reshape2)
library(fable)
library(forecast)
library(foreign)
library(fpp2)
library(stats)
library(tseries)
library(astsa)
library(quantmod)
library(urca)
```
### helper functions
```{r}
# Function to analyze data range, check for duplicates, and missing values
analyze_data_quality <- function(data_frame) {
  # Data range
  date_range <- data_frame %>%
    summarize(
      min_datetime = min(datetime),
      max_datetime = max(datetime)
    )
  
  min_date <- format(date_range$min_datetime, format = "%Y-%m-%d %H:%M:%S")
  max_date <- format(date_range$max_datetime, format = "%Y-%m-%d %H:%M:%S")
  
  cat("Data is available in the date range from",
      min_date,
      "to",
      max_date, "\n")
  
  # Check for duplicates
  if (any(duplicated(data_frame))) {
    cat("There are duplicates in the dataset.\n")
  } else {
    cat("There are no duplicates in the dataset.\n")
  }
  
  # Check for missing values
  if (any(is.na(data_frame))) {
    cat("There are missing values in the dataset.\n")
  } else {
    cat("There are no missing values in the dataset.\n")
  }
}
```


# Load data

- **Telemetry Time Series Data** (PdM_telemetry.csv): It consists of hourly average of voltage, rotation, pressure, vibration collected from 100 machines for the year 2015.
- **Errors** (PdM_errors.csv): These are errors encountered by the machines while in operating condition. Since, these errors don't shut down the machines, these are not considered as failures. The error date and times are rounded to the closest hour since the telemetry data is collected at an hourly rate.
- **Maintenance** (PdM_maint.csv): If a component of a machine is replaced, that is captured as a record in this table. Components are replaced under two situations:
  - During the regular scheduled visit, the technician replaced it (Proactive Maintenance)
  - A component breaks down and then the technician does an unscheduled maintenance to replace the component (Reactive Maintenance). This is considered as a failure and corresponding data is captured under Failures. Maintenance data has both 2014 and 2015 records. This data is rounded to the closest hour since the telemetry data is collected at an hourly rate.
- **Failures** (PdM_failures.csv): Each record represents replacement of a component due to failure. This data is a subset of Maintenance data. This data is rounded to the closest hour since the telemetry data is collected at an hourly rate.
- **Metadata of Machines** (PdM_Machines.csv): Model type & age of the Machines.

```{r}
pdm_telemetry <- read_csv(
  "data/PdM_telemetry.csv",
  col_types = cols(
    datetime = col_datetime(format = "%Y-%m-%d %H:%M:%S"),
    machineID = col_integer(),
    volt = col_double(),
    rotate = col_double(),
    pressure = col_double(),
    vibration = col_double()
  )
)
pdm_errors <- read_csv(
  "data/PdM_errors.csv",
  col_types = cols(
    datetime = col_datetime(format = "%Y-%m-%d %H:%M:%S"),
    machineID = col_integer(),
    errorID = col_character()
  )
)
pdm_maint <- read_csv(
  "data/PdM_maint.csv",
  col_types = cols(
    datetime = col_datetime(format = "%Y-%m-%d %H:%M:%S"),
    machineID = col_integer(),
    comp = col_character()
  )
)
pdm_failures <- read_csv(
  "data/PdM_failures.csv",
  col_types = cols(
    datetime = col_datetime(format = "%Y-%m-%d %H:%M:%S"),
    machineID = col_integer(),
    failure = col_character()
  )
)
pdm_machines <- read_csv(
  "data/PdM_machines.csv",
  col_types = cols(
    machineID = col_integer(),
    model = col_character(),
    age = col_integer()
  )
)
```

```{r}
pdm_telemetry <- pdm_telemetry %>%
  arrange(datetime, machineID)

pdm_errors <- pdm_errors %>%
  arrange(datetime, machineID)

pdm_maint <- pdm_maint %>%
  arrange(datetime, machineID)

pdm_failures <- pdm_failures %>%
  arrange(datetime, machineID)
```

# EDA: Telemetry
```{r}
glimpse(pdm_telemetry)
head(pdm_telemetry)
```

### n_machines
```{r}
print("The number of unique machines in the pdm_telemetry dataset is:")
print(pdm_telemetry %>%
    distinct(machineID) %>%
    nrow()
)
```

### Data Quality Check
```{r}
analyze_data_quality(pdm_telemetry)
```

### Correlation and Distribution Scatterplot Matrix
```{r}
ggpairs(pdm_telemetry, columns = c("volt", "rotate", "pressure", "vibration"))
```

### Visualize Machine 1 Sensor Data
```{r}
# Filter and select data for Machine 1
df_machine_1 <- pdm_telemetry %>%
  dplyr::filter(machineID == 1) 

# Reshape the data to long format
df_machine_1_long <- df_machine_1 %>%
  dplyr::select(datetime, volt, rotate, pressure, vibration) %>%
  tidyr::pivot_longer(cols = c(volt, rotate, pressure, vibration), names_to = "variable", values_to = "value")

# Create a facetted line plot
ggplot(df_machine_1_long, aes(x = datetime, y = value, color = variable)) +
  geom_line(size = 1) +
  labs(title = "Variables for Machine 1", xlabel = "Time", ylabel = "Value") +
  facet_wrap(~variable, scales = "free_y", ncol = 1)
```

### Observations about Telemetry Data
- This may be synthetically generated data distributed between 1st Jan 2015 to 1st Jan 2016.
- Each row represents the state of a machine on a particular hour. Voltage, vibration, pressure & rotation of a machine have been averaged hourly.
- There are 100 unique Machines.
- There are no duplicates or missing values in the pdm_telemetry dataset.
- The four parameters voltage, vibration, pressure & rotation are normally distributed.

# EDA: Error
This data includes the errors encountered by the machines while in operating condition. Since, these errors don't shut down the machines, these are not considered as failures. The error date and times are rounded to the closest hour since the telemetry data is collected at an hourly rate.
```{r}
glimpse(pdm_errors)
head(pdm_errors)
```

### n_unique_errors
```{r}
unique_errors <- pdm_errors %>%
  distinct(errorID) %>%
  nrow()

print("The number of unique errors in the pdm_errors dataset is:")
print(unique_errors)
```

### Data Quality Check
```{r}
analyze_data_quality(pdm_errors)
```

### Bar Plot of Different Error Types
```{r}
# Create a bar plot for error types
plot <- ggplot(pdm_errors, aes(x = errorID)) +
  geom_bar() +
  labs(
    x = "Error Type",
    y = "Count",
    title = "Different Types of Errors"
  ) +
  theme_minimal()

print(plot)
```

### Bar Plot of Errors Across MachineID
```{r}
# Create a bar plot for errors across MachineID
plot <- ggplot(pdm_errors, aes(x = machineID)) +
  geom_bar() +
  coord_flip() +
  labs(
    x = "MachineID",
    y = "Count",
    title = "Number of Errors Across MachineID"
  ) +
  theme_minimal()

print(plot)
```

### Machine-to-Error Distribution
```{r}
# Group by machineID and errorID, count occurrences, and reset column names
df_errors <- pdm_errors %>%
  group_by(machineID, errorID) %>%
  summarise(errorValues = n()) %>%
  ungroup()

# Create a stacked bar plot for machine-to-error distribution
plot <- ggplot(df_errors, aes(x = factor(machineID), y = errorValues, fill = errorID)) +
  geom_bar(stat = "identity", position = "stack", width = 0.8) +
  labs(
    x = "Machine ID",
    y = "Number of Errors",
    title = "Count of Errors for Different Machines"
  ) +
  theme_minimal() +
  theme(legend.position = "top")  # Move the legend to the top

print(plot)
```

### Plot Number of Errors Across Machines Over Days
```{r}
# Count errors by date and sort by index
errors_count <- table(as.Date(pdm_errors$datetime))

# Create a time series plot for the number of errors across days
plot <- ggplot(data.frame(date = as.Date(names(errors_count)), count = as.numeric(errors_count)), aes(x = date, y = count)) +
  geom_line(size = 1) +
  labs(
    x = "Time",
    y = "Number of Errors",
    title = "Number of Errors Across Days"
  ) +
  theme_minimal()

print(plot)
```

### Distribution of the Number of Errors Per Day Across Machines
```{r}
# Convert datetime to date in the pdm_errors
pdm_errors$date <- as.Date(pdm_errors$datetime)

# Group by date, count occurrences, and create a histogram
errors_per_day <- pdm_errors %>%
  group_by(date) %>%
  summarise(ErrorCount = n())

# Create a histogram of the number of errors per day
plot <- ggplot(errors_per_day, aes(x = ErrorCount)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black") +
  labs(
    x = "Number of Errors on a Particular Day",
    y = "Frequency",
    title = "Distribution of Number of Errors Per Day"
  ) +
  theme_minimal()

print(plot)
```

# EDA: Maintenance
If a component of a machine is replaced, that is captured as a record in this table. Components are replaced under two situations:
- During the regular scheduled visit, the technician replaced it (Proactive Maintenance)
- A component breaks down and then the technician does an unscheduled maintenance to replace the component (Reactive Maintenance). This is considered as a failure and corresponding data is captured under Failures. Maintenance data has both 2014 and 2015 records. This data is rounded to the closest hour since the telemetry data is collected at an hourly rate.
```{r}
glimpse(pdm_maint)
head(pdm_maint)
```

### Data Quality Check
```{r}
analyze_data_quality(pdm_maint)
```
Maintenance data is present June 2014 onwards. This is different from other data which are present between 2015 and 2016.

### Plot Number of Maintenance Records Across Months
```{r}
# Extract month and year from datetime column
pdm_maint <- pdm_maint %>%
  mutate(month_year = format(datetime, "%Y-%m"))

# Create a bar plot for the number of maintenance records across months and years
plot <- ggplot(pdm_maint, aes(x = month_year)) +
  geom_bar() +
  labs(
    x = "Month-Year",
    y = "Number of Maintenance Records",
    title = "Number of Maintenance Records Across Months and Years"
  ) +
  theme_minimal()

print(plot)
```
Number of components replaced in the year 2015 are considerably higher compared to the 2014.

### Components Replaced
```{r}
# Create a bar plot for the number of components replaced
plot <- ggplot(pdm_maint, aes(x = comp)) +
  geom_bar() +
  labs(
    x = "Component",
    y = "Count",
    title = "Components Replaced"
  ) +
  theme_minimal()

print(plot)
```
Four types components are replaced almost in the same numbers.

### Number of Maintenance Records Across Machines
```{r}
# Create a bar plot for the number of maintenance records across machine IDs
plot <- ggplot(pdm_maint, aes(x = factor(machineID))) +
  geom_bar() +
  coord_flip() +
  labs(
    x = "Machine ID",
    y = "Number of Maintenance Records",
    title = "Number of Maintenance Records Across Machine IDs"
  ) +
  theme_minimal()

print(plot)
```

### Machine to Components Replaced
```{r}
# Group by machineID and comp, count occurrences, and reset column names
df_maint <- pdm_maint %>%
  group_by(machineID, comp) %>%
  summarise(num_comp = n()) %>%
  ungroup()

# Create a stacked bar plot for machine-to-components replaced
plot <- ggplot(df_maint, aes(x = factor(machineID), y = num_comp, fill = comp)) +
  geom_bar(stat = "identity", position = "stack", width = 0.8) +
  labs(
    x = "Machine ID",
    y = "Number of Components Replaced",
    title = "Count of Components Replaced for Different Machines"
  ) +
  theme_minimal() +
  theme(legend.position = "top")  # Move the legend to the top

print(plot)
```

### Number of Maintenance Issues Raised Per Day
```{r}
# Count the number of maintenance records per day and create a time series plot
plot <- ggplot(data = pdm_maint, aes(x = as.Date(datetime))) +
  geom_point(aes(y = after_stat(count)), stat = "count", color = "black") +
  labs(
    x = "Time",
    y = "Number of Maintenance Records",
    title = "Number of Maintenance Records Across Time"
  ) +
  theme_minimal()

print(plot)
```
This indicates that there is a drastic difference between the number of maintenance records in 2014 vs 2015.

#EDA machines
This data set includes some information about the machines: model type and age (years in service).
```{r}
glimpse(pdm_machines)
head(pdm_machines)
```

### Distribution of Machine Ages
```{r}
# Create a box plot for the distribution of machine ages
plot <- ggplot(pdm_machines, aes(y = age)) +
  geom_boxplot() +
  labs(
    y = "Machine Age",
    title = "Distribution of Machine Ages"
  ) +
  theme_minimal()

print(plot)
```
The age of the Machines is distributed between 0 to 20. The median age is to ~12.5. There are no outliers. Another indication that this is a synthetic data.

### Machine Age Distribution
```{r}
# Create a plotly figure
fig <- plot_ly()

# Add histogram traces for each model
fig <- fig %>% add_trace(
  data = pdm_machines %>%
    filter(model == "model1"),
  x = ~age,
  name = "model1",
  type = "histogram"
)

fig <- fig %>% add_trace(
  data = pdm_machines %>%
    filter(model == "model2"),
  x = ~age,
  name = "model2",
  type = "histogram"
)

fig <- fig %>% add_trace(
  data = pdm_machines %>%
    filter(model == "model3"),
  x = ~age,
  name = "model3",
  type = "histogram"
)

fig <- fig %>% add_trace(
  data = pdm_machines %>%
    filter(model == "model4"),
  x = ~age,
  name = "model4",
  type = "histogram"
)

# Customize layout
fig <- fig %>% layout(
  xaxis = list(title = "Age"),
  yaxis = list(title = "Count"),
  barmode = "stack",
  title = "Machine Age Distribution"
)

# Display the plot
fig
```

#EDA failures
```{r}
glimpse(pdm_failures)
head(pdm_failures)
```

### Data Quality Check
```{r}
analyze_data_quality(pdm_failures)
```

### Count of Failures
```{r}
# Create a plotly bar chart
fig <- plot_ly(pdm_failures, x = ~failure, type = "histogram") %>%
  layout(
    title = "Count of Failures",
    xaxis = list(title = "Failure Type"),
    yaxis = list(title = "Count"),
    template = "plotly_dark"
  )

# Display the plot
fig
```

# Predictive Model
```{r}
combined_data <- pdm_telemetry %>%
  full_join(pdm_failures, by = c("machineID", "datetime"))
# Create a summary of the number of days with and without failures
combined_data <- combined_data %>%
  mutate(has_failure = datetime %in% pdm_failures$datetime)
summary_data <- combined_data %>%
  group_by(has_failure) %>%
  summarise(count = n())
```

```{r}
ggplot(summary_data, aes(x = factor(has_failure), y = count, fill = factor(has_failure))) +
  geom_bar(stat = "identity") +
  scale_fill_manual(values = c("red", "green")) + 
  labs(x = "Machine failure", y = "Number of Days") +
  theme_minimal()
```

```{r}
total_failures <- sum(combined_data$has_failure)
total_failures
```
```{r}
library(lubridate)

# Suppose datetime is a column of combined_data
# Extract datetime components
combined_data$year <- year(combined_data$datetime)
combined_data$month <- month(combined_data$datetime)
combined_data$day <- day(combined_data$datetime)
combined_data$hour <- hour(combined_data$datetime)

# Create a new column for the day of the week
combined_data$weekday <- wday(combined_data$datetime, label = TRUE)

# Create a new column for the day of the year
combined_data$day_of_year <- yday(combined_data$datetime)

```
```{r}

# Filter rows with non-null values in the variable 'failure'.
filtered_data <- combined_data[!is.na(combined_data$failure), ]

# Group data by month and count total failures
monthly_failures <- filtered_data %>%
  group_by(month) %>%
  summarise(total_failures = sum(!is.na(failure)))

# Create a seasonal chart for total failures
ggplot(monthly_failures, aes(x = month, y = total_failures)) +
  geom_bar(stat = "identity", fill = "lightcoral") +
  labs(title = "Seasonal Pattern of Total Failures",
       x = "Month", y = "Number of Total Failures") +
  theme_minimal()


```

```{r}
combined_data <- combined_data %>% 
  mutate(failure_indicator = ifelse(!is.na(failure), 1, 0)) %>%
  group_by(datetime) %>%
  summarise(total_failures = sum(failure_indicator))
```

```{r}
# Create a time series of total_failures with hourly frequency (24) and set the start time.
total_failures.ts <- ts(combined_data$total_failures, 
                        start = c(2015, 1, 1, 6),  # Adjust the start-up to January 1, 2015
                        frequency = 1)

total_failures.ts
```
```{r}
plot(total_failures.ts)
adf.test(total_failures.ts)
```
```{r}
total_failures_log <- log(total_failures.ts)
total_failures_log <- total_failures_log[!is.na(total_failures_log) & is.finite(total_failures_log)]

# Check if there is enough non-NA and finite data to perform the ADF test.
if (length(total_failures_log) > 0) {
  plot(total_failures_log)
  
  # Perform the ADF test
  ur.df_result <- ur.df(total_failures_log, type = "none", selectlags = "AIC")
  summary(ur.df_result)
} else {
  cat("There is insufficient non-NA and finite data to perform the ADF test.\n")
}
```
```{r}
modelT=auto.arima(total_failures.ts)
modelT
summary(modelT)
```
```{r}
par(mar=c(5, 4, 2, 2))
tsdiag(modelT)
```
```{r}
#Ruido blanco
Box.test(residuals(modelT), type = "Ljung-Box")
error_modelT=residuals(modelT)
plot(error_modelT)
```
```{r}
#Realizamos la predicción
predictT <- forecast::forecast(modelT, h = 30)
predictT
plot(predictT)
```
